![alt text](https://i.imgur.com/lQJjpAk.gif)


This thread concerns attempts to construct artificial general intelligence, which I often underline [may likely be mankind's last invention.](https://www.youtube.com/watch?v=9snY7lhJA4c)

I clearly unravel how I came to invent the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning), (a component in another description called ['thought curvature'](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)) in relation to quantum computation.

I am asking anybody that knows [supermathematics](https://en.wikipedia.org/wiki/Supermathematics) and machine learning to pitch in the discussion below.

Part A - What is the goal?
======
(1) The aim is to build [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence).

(2) Machine learning often concerns [constraining algorithms with respect to biological examples](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/).

**(3)** Babies are great  examples of **some non-trivial basis** for [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence); babies are significant examples of biological baseis that are reasonably usable to inspire smart algorithms, especially in the aims of (1), regarding (2).

[Thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) together with the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning), espouses the **importance** of [considering biological constraints](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/) in the aim of developing  [general machine learning models](https://en.wikipedia.org/wiki/Artificial_general_intelligence), pertinently, where [babies&#39; brains are observed to be pre-equipped with particular &quot;**physics priors**&quot;, constituting specifically, the ability for babies to intuitively know laws of physics, while learning by reinforcement](http://science.sciencemag.org/content/348/6230/91).

The phrasing “intuitively know laws of physics”, should not be confused for babies are nobel laureate or physics undergrad-babies, that do physics exams, but instead, that babies' brains are pre-baked with ways to naturally exercise physics based expectations w.r.t. interactions with objects in their world, as indicated by [Stahl et al](http://science.sciencemag.org/content/348/6230/91)).

The importance of **recognizing underlying causal physics laws in learning models** (although **not via supermanifolds** , as encoded in [Thought Curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)), has recently been both [demonstrated](https://arxiv.org/abs/1606.05579) and separately [echoed by Deepmind](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/), and of late, distinctly [emphasized](https://arxiv.org/abs/1709.08568) by Yoshua Bengio.




Part B - Babies know physics, plus they learn
======
Back in 2016, I [read somewhere that babies know some physics intuitively](https://www.washingtonpost.com/news/speaking-of-science/wp/2015/04/02/new-study-reveals-the-shockingly-complex-thought-processes-of-infants/?utm_term=.dd0b9545030b). 

Also, it is empirically observable that babies use that intuition to develop abstractions of knowledge, [in a reinforcement learning like manner](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3490621/).



Part C - Algorithms for reinforcement learning and physics
======
Now, I knew beforehand of two types of major deep learning models, that:

(1) used reinforcement learning. ([Deepmind Atari q](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf))

(2) learn laws of physics. ([Uetorch](https://github.com/facebook/UETorch))

**However**:

(a) Object detectors like [(2)](https://github.com/facebook/UETorch) use something called [pooling](http://iamaaditya.github.io/2016/03/one-by-one-convolution/) to gain translation invariance over objects, so that the model learns regardless of where the object in the image is positioned.

(b) Instead, [(1)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  excludes pooling, because [(1)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  requires translation variance, in order for Q learning to apply on the changing positions of the objects in pixels.


Part D - Problem discussion...
======
As a result I sought a model that could deliver both translation invariance and variance at the same time, and reasonably, **part of the solution** was models that disentangled factors of variation, i.e. [manifold learning frameworks](https://arxiv.org/abs/1611.03383).

I didn't stop my scientific thinking at [manifold learning](http://scikit-learn.org/stable/modules/manifold.html) though.

Given that cognitive science may be used to constrain machine learning models ([similar to how firms like Deepmind often use cognitive science as a boundary on the deep learning models they produce](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)) I sought to create a disentanglable model that was as constrained by cognitive science, as far as algebra would permit.



Part E - Problem approach...
======
As a result I created something called the [supermanifold hypothesis in deep learning](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning). (A part of a system called ['thought curvature'](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)). 

This was due to [evidence of supersymmetry in cognitive science](https://arxiv.org/abs/0705.1134); I compacted machine learning related algebra for disentangling, in the regime of [supermanifolds](https://en.wikipedia.org/wiki/Supermanifold). This could be seen as an extension of [manifold learning in artificial intelligence](http://scikit-learn.org/stable/modules/manifold.html).

**Given that the [supermanifold hypothesis](https://www.researchgate.net/publication/316617464_Supermanifold_Hypothesis_via_Deep_Learning) compounds** ϕ(x,![alt text](https://i.imgur.com/PRSAGxn.png),![alt text](https://i.imgur.com/ncrjUdkm.png))<SUP>T</SUP>w, **here is an annotation of the hypothesis:**


1. [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) entails ϕ(x;![alt text](https://i.imgur.com/PRSAGxn.png))<SUP>T</SUP>w, that denotes the input space x, and learnt representations ![alt text](https://i.imgur.com/PRSAGxn.png).
2. [Deep Learning](https://en.wikipedia.org/wiki/Deep_learning) underlines that coordinates or latent spaces in the [manifold](https://en.wikipedia.org/wiki/Manifold) framework, are learnt features/representations, or directions that are sparse configurations of coordinates.
3. [Supermathematics](https://en.wikipedia.org/wiki/Supermathematicsg) entails (x,![alt text](https://i.imgur.com/PRSAGxn.png),![alt text](https://i.imgur.com/ncrjUdkm.png)), that denotes some x valued coordinate distribution, and by extension, directions that compact coordinates via ![alt text](https://i.imgur.com/PRSAGxn.png),![alt text](https://i.imgur.com/ncrjUdkm.png)
4.  As such, the aforesaid (x,![alt text](https://i.imgur.com/PRSAGxn.png),![alt text](https://i.imgur.com/ncrjUdkm.png)), is subject to coordinate transformation.
5. Thereafter 1, 2, 3, 4 and [supersymmetry in cognitive science](https://arxiv.org/abs/0705.1134), within the generalizable nature of [euclidean space](https://en.wikipedia.org/wiki/Euclidean_space), reasonably effectuate ϕ(x,![alt text](https://i.imgur.com/PRSAGxn.png),![alt text](https://i.imgur.com/ncrjUdkm.png))<SUP>T</SUP>w.


Part F - A probable experiment: A Transverse Field Ising Spin (Super)–Hamiltonian Quantum Computation 
=====
![alt text](https://i.imgur.com/scLHFT7.png)


Part G - Limitations
====

**Although** [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) is minor particularly in its simple description (acquiescing [SQCD](https://arxiv.org/abs/1104.1425)) in relation to [Artificial General Intelligence](https://en.wikipedia.org/wiki/Artificial_General_Intelligence), it **crucially** delineates that the math of [supermanifolds](https://en.wikipedia.org/wiki/Supermanifold) is reasonably applicable in [Deep Learning](https://en.wikipedia.org/wiki/Deep_Learning), imparting that [cutting edge  Deep Learning work tends to consider boundaries in the biological brain](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence/) while underscoring that biological brains can be **optimally** evaluated using [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) operations.

In broader words, [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) occurs on the following evidence:

1. [Manifolds](https://en.wikipedia.org/wiki/Manifold) are in the regime of very [general algorithms](http://scikit-learn.org/stable/modules/manifold.html), that enable models to **learn many degrees of freedom** in latent space, (i.e. size, scale etc… where said degrees are observable as features of **physics** interactions) where transformations on points may represent for e.g., features of a particular object in pixel space, and transformations on said points or weights of an object are **disentangleable** or separable from those pertaining to other objects in latent space. ([Early Visual Concept Learner](https://arxiv.org/abs/1606.05579), [Mean field theory expressivity networks](https://arxiv.org/abs/1606.05340), etc)

2. Given (1), and the generalizability of  [euclidean space](https://en.wikipedia.org/wiki/Euclidean_space), together with the instance that there persists  [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) measurements in biological brains, thought curvature predicates that  [Supermathematics](https://en.wikipedia.org/wiki/Supermathematics) or  [Lie Superalgebras](https://en.wikipedia.org/wiki/Lie_Superalgebras) (in  [Supermanifolds](https://en.wikipedia.org/wiki/Supermanifold)) may reasonably, empirically apply in  [Deep Learning](https://en.wikipedia.org/wiki/Deep_Learning), or some other named study of hierarchical learning in research.

Part H - A brief discussion on the significance of a Transverse Field Ising Spin (Super)-Hamiltonian learning algorithm
======
The usage of [supersymmetric](https://en.wikipedia.org/wiki/Supersymmetry) operations is **imperatively** efficient, as such operations enable **deeply abstract representations** (as is naturally afforded by symmetry group Lie Superalgebras (See [source 1](https://arxiv.org/abs/0705.1134), [source 2](https://en.wikipedia.org/wiki/Symmetry_group#See_also))), pertinently, in a [general, biologically tenable time-space complex optimal regime](https://arxiv.org/abs/0705.1134). 

As such, said **deeply abstract representations** may **reasonably capture** certain **“physics priors”**, (See **page 4** of the [paper](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis)) abound the [laws of physics](https://en.wikipedia.org/wiki/Laws_of_science).  


Part I - Considerations
======
Notably, an ***initial degree*** of the
[(Super-) Hamiltonian](https://arxiv.org/abs/hep-th/0506170) structure required by [thought curvature](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) shall require a quite scalable scheme, such as some [boson sampling](https://en.wikipedia.org/wiki/Boson_sampling) aligned range, in conjunction with [supersymmetric space](https://arxiv.org/abs/0705.1134). This scheme is approachable [on the scale of 42 qubits](https://www.researchgate.net/publication/316643055_No_imminent_quantum_supremacy_by_boson_sampling#pf6), or a 42 qubit = ![alt text](https://i.imgur.com/qTHl2uA.png) gb = 131,072 gb ram configuration for simple task/circuit tests.

Separately, I am working to determine how feasible the  [model](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis) is.

I am working to design suitable experiments, and figuring out what type of ![alt text](https://i.imgur.com/4P5rY64.png) (training samples) are sufficiently applicable to the [model](https://www.researchgate.net/publication/316586028_Thought_Curvature_An_underivative_hypothesis).

Remember, if you have good knowledge of [supermathematics](https://en.wikipedia.org/wiki/Supermathematics) and machine learning, you may pitch in for a discussion, by messaging me at jordanmicahbennett@gmail.com.

Part J - Extras
======

I compiled a list of resources (beyond things cited throughout the papers) that may be helpful [here](https://github.com/JordanMicahBennett/Supermathematics-and-Artificial-General-Intelligence/blob/master/1.%20Extra%20list%20of%20helpful%20resources.md).



Part K - Article Download
======
Here is this article in [pdf form](https://www.researchgate.net/publication/319523372_Supermathematics_and_Artificial_General_Intelligence).


